{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝을 위한 PyTorch 활용법\n",
    "## Chapter 2. Linear Regression & Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Variable & Automatic Gradient Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor = torch.Tensor(3,4)\n",
    "x_tensor.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 위에 생성한 tensor와 똑같아 보이지만 Variable이라는 클라쓰에 감싸져있음\n",
    "# 현재 버젼(4.1)에는 그냥 tensor도 Variable이랑 같음(기능 추가)\n",
    "x_variable = Variable(x_tensor)\n",
    "print(x_variable)\n",
    "print(x_variable.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# .grad -> 해당 tensor(or variable)에 대한 gradient를 출력한다.\n",
    "print(x_variable.grad) # 연산이 없기 때문에 None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requires Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad -> gradient를 만들지 여부를 설정한다.\n",
    "print(x_variable.requires_grad)\n",
    "\n",
    "x_variable = Variable(x_tensor, requires_grad = True) # 여부 True\n",
    "print(x_variable.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volatile mode(지금은 바뀜)\n",
    "- 불필요한 계산 없이 테스트 용으로 전환해준다.(Inference mode)\n",
    "- 결과를 빠르게 알 수 있다.\n",
    "- 하나의 variable에 volatile = True가 있으면 전체에 적용된다.\n",
    "- **volatile은 없어지고 torch.no_grad()로 바뀜**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.grad_mode.no_grad'>\n",
      "None False\n"
     ]
    }
   ],
   "source": [
    "x_variable = Variable(x_tensor)\n",
    "print(torch.no_grad)\n",
    "print(x_variable.grad, x_variable.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph & Variable\n",
    "- 연산그래프 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor(3,4), requires_grad = True)\n",
    "y = x**2 + 4*x\n",
    "z = 2*y + 3\n",
    "x.requires_grad, y.requires_grad, z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# backward(gradient, retain_graph, create_graph, retain_variables)\n",
    "# 역전파를 자동으로 해주는 함수\n",
    "# gradient를 계산하는데 현재 존재하는 변수들의 leaves에는 저장하겠다.\n",
    "# 따라서 x에만 기울기가 있고 나머지 y, z에는 없다. \n",
    "\n",
    "loss = torch.FloatTensor(3,4) # 임의로 랜덤값으로 초기화함\n",
    "z.backward(loss) # z값에 대하여 역전파를 하여라, loss값은 loss가 나왔다.\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "\n",
    "# y,z가 모두 x로 표현될 수 있기 때문에"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
